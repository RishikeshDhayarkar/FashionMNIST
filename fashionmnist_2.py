# -*- coding: utf-8 -*-
"""fashionMNIST_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-VxKu0seEAtTAeFz5OwPdqsKMTEgH7Ow
"""

import torch 
import torch.nn as nn 
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim

torch.set_grad_enabled(True)

def get_num_correct(preds, labels):
  return preds.argmax(dim=1).eq(labels).sum().item()

class Network(nn.Module):
  def __init__(self):
    super(Network, self).__init__()
    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)

    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)
    self.fc2 = nn.Linear(in_features=120, out_features=60)
    self.out = nn.Linear(in_features=60, out_features=10)

  def forward(self, t):
    # (1) Input layer
    t=t

    # (2) Hidden conv layer
    t=self.conv1(t)
    t=F.relu(t)
    t=F.max_pool2d(t, kernel_size=2, stride=2)

    # (3) Hidden conv layer
    t=self.conv2(t)
    t=F.relu(t)
    t=F.max_pool2d(t, kernel_size=2, stride=2)

    # (4) Hidden Linear layer
    t=t.reshape(-1, 12*4*4)
    t=self.fc1(t)
    t=F.relu(t)

    # (5) Hidden Linear layer
    t=self.fc2(t)
    t=F.relu(t)

    # (6) Output layer
    t=self.out(t)
    
    return t

train_set = torchvision.datasets.FashionMNIST(root='./data/FashionMNIST',
                                              train=True, 
                                              download=True, 
                                              transform=transforms.Compose([transforms.ToTensor()]))

"""<h2>Training with a single batch</h2>"""

network = Network()

train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)
optimizer = optim.Adam(network.parameters(), lr=0.01)


for epoch in range(5):
  
  total_loss = 0
  total_correct = 0

  for batch in train_loader:
    images, labels = batch

    preds = network(images)
    loss = F.cross_entropy(preds, labels) # Calculate loss
    
    optimizer.zero_grad()
    loss.backward() #Calculate gradients
    optimizer.step() #Update weights

    total_loss += loss.item()
    total_correct += get_num_correct(preds, labels)

  print('epoch:', epoch, 'total_correct:', total_correct, 'loss:', total_loss)

total_correct / len(train_set)

"""Getting predictions for the complete training set, without gradients and backpropagation. The code till here trains the model using gradients and backprop but now that we want to just get the predictions we can turn the gradient tracking feature off. The weights that were calculated during the 5th epoch are still available in the 'Parameters' of the netowork layers. So, by turning the gradient tracking feature off we are using the previously generated weights to make new predictions. By turning the gradient tracking off the amount of computational resources used will be less."""

def get_all_preds(model, loader):
  all_preds = torch.tensor([])
  for batch in loader:
    images, labels = batch
    preds = model(images)
    all_preds = torch.cat((all_preds, preds), dim=0)
  return all_preds

prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)
train_preds = get_all_preds(network, prediction_loader)

train_preds.shape

print(train_preds.requires_grad) # Tells if gradient tracking feature is enabled or not. Here this returns True because gradient tracking is enabled globally.

train_preds.grad # Generates no output because there no backpropagation

train_preds.grad_fn # Since the gradient tracking feature is turned on initially(in the imports section) pytorch tries to keep track of the gradient
# tensor, even though the gradient tensor is empty

"""There are 2 ways of turning off gradients 1) Turn it of globally(check the import cell)

2) Turn if locally with 'torch.no_grad()' function
"""

with torch.no_grad():
  prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)
  train_preds = get_all_preds(network, prediction_loader)

train_preds.shape

print(train_preds.requires_grad) # Tells if gradient tracking feature is enabled or not. Here this returns False because gradient tracking is disabled locally.

train_preds.grad # Generates no output because there no backpropagation

train_preds.grad_fn # Generates no output because pytorch is not keeping track of the gradient tensor

preds_correct = get_num_correct(train_preds, train_set.targets)
print('Total correct predictions = ', preds_correct)
print('Accuracy = ', preds_correct/ len(train_set))

"""Building a confusion matrix manually(without using the function from sklearn)"""

train_preds.shape

train_preds.argmax(dim=1)

train_set.targets

train_preds.argmax(dim=1).shape

train_set.targets.shape

stacked = torch.stack((train_set.targets, train_preds.argmax(dim=1)), dim=1)

stacked.shape

stacked

cmat = torch.zeros(10, 10, dtype=torch.int64)
cmat

for pair in stacked:
  tar, pr = pair.tolist()
  cmat[tar, pr] = cmat[tar, pr]+1

cmat

"""Creating confusion matrix using a built-in function"""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))

cm